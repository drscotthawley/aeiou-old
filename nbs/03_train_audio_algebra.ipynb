{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52056925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_audio_algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898d0fb",
   "metadata": {},
   "source": [
    "# train_audio_algebra\n",
    "\n",
    "> Trying to map audio embeddings to vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from prefigure.prefigure import get_all_args, push_wandb_config\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import json\n",
    "\n",
    "import accelerate\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "#from torch.utils import data\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import wandb\n",
    "from shazbot.viz import embeddings_table, pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image\n",
    "import shazbot.blocks_utils as blocks_utils\n",
    "from shazbot.icebox import load_audio_for_jbx, IceBoxEncoder\n",
    "from shazbot.data import MultiStemDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2160219",
   "metadata": {},
   "source": [
    "## The main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class AudioAlgebra(nn.Module):\n",
    "    def __init__(self, global_args, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = IceBoxEncoder(global_args)\n",
    "        self.dims = global_args.latent_dims\n",
    "        \n",
    "        embed_block = nn.Sequential([\n",
    "            nn.Linear(self.dims,self.dims),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm()\n",
    "            ])\n",
    "        \n",
    "        self.reembedding = nn.Sequential([  # something simple at first\n",
    "            embed_block,\n",
    "            embed_block,\n",
    "            embed_block,\n",
    "            embed_block,\n",
    "            embed_block,\n",
    "            nn.Linear(self.dims,self.dims)\n",
    "            ])\n",
    "        \n",
    "    def forward(self, \n",
    "        stems:list,   # list of torch tensors denoting solo audio parts to be mixed together\n",
    "        faders:list   # list of gain values to be applied to each stem\n",
    "        ):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            zs, zsum = [], torch.zeros((self.dims)).float()\n",
    "            mix = torch.zeros_like(stems[0]).float()\n",
    "            for s, f in zip(stems, faders):\n",
    "                mix_s = s * f             # audio stem adjusted by gain fader f\n",
    "                with torch.no_grad():\n",
    "                    z = self.encoder.encode(mix_s).float()  # initial/frozen embedding/latent for that input\n",
    "                z = self.reembedding(z).float()   # <-- this is the main work of the model \n",
    "                zsum += z                 # compute the sum of all the z's\n",
    "                mix += mix_s              # save a record of full audio mix\n",
    "                zs.append(z)              # save a list of individual z's\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                zmix = self.encoder.encode(mix).float()  # compute embedding / latent for the full mix\n",
    "\n",
    "        return zsum, zmix, zs, mix    # zsum = pred, zmix = target,  zs & zmix are just for extra info\n",
    "        \n",
    "        \n",
    "    def distance(self, pred, targ):\n",
    "            return torch.norm( pred - targ ) # L2 / Frobenius / Euclidean\n",
    "        \n",
    "    def loss(self, zsum, zmix):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = distance(zsum, zmix)\n",
    "        log_dict = {'train/loss': loss.detach()}\n",
    "        return loss, log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a940aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def demo():\n",
    "    print(\"In demo placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fad0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def get_stems_faders(batch, dl):\n",
    "    \"grab some more audio stems and set faders\" \n",
    "    nstems = 1 + int(torch.randint(5,(1,1))[0][0].numpy())\n",
    "    faders = 2*torch.rand(nstems)-1  # fader gains can be from -1 to 1\n",
    "    stems = [batch]\n",
    "    train_iter = iter(train_dl)\n",
    "    for i in range(nstems-1):\n",
    "        stems.append(next(train_iter))\n",
    "    return stems, faders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ac46f",
   "metadata": {},
   "source": [
    "## Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def main():\n",
    "\n",
    "    args = get_all_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method(args.start_method)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "     \n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    print('Using device:', device, flush=True)\n",
    "\n",
    "    aa_model = AudioAlgebra(args, device)\n",
    "\n",
    "    accelerator.print('Parameters:', blocks_utils.n_params(aa_model))\n",
    "\n",
    "    # If logging to wandb, initialize the run\n",
    "    use_wandb = accelerator.is_main_process and args.name\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        config = vars(args)\n",
    "        config['params'] = utils.n_params(aa_model)\n",
    "        wandb.init(project=args.name, config=config, save_code=True)\n",
    "\n",
    "    opt = optim.Adam([*aa_model.reembedding.parameters()], lr=4e-5)\n",
    "\n",
    "    train_set = MultiStemDataSet([args.training_dir], args)\n",
    "    train_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=True,\n",
    "                               num_workers=args.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "    aa_model, opt, train_dl = accelerator.prepare(aa_model, opt, train_dl)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.watch(aa_model)\n",
    "        \n",
    "    if args.ckpt_path:\n",
    "        ckpt = torch.load(args.ckpt_path, map_location='cpu')\n",
    "        accelerator.unwrap_model(aa_model).load_state_dict(ckpt['model'])\n",
    "        opt.load_state_dict(ckpt['opt'])\n",
    "        epoch = ckpt['epoch'] + 1\n",
    "        step = ckpt['step'] + 1\n",
    "        del ckpt\n",
    "    else:\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "\n",
    "    \n",
    "    # training loop    \n",
    "    try:\n",
    "        while True:\n",
    "            for batch in tqdm(train_dl, disable=not accelerator.is_main_process):\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                stems, faders = get_stems_faders(batch, train_dl)\n",
    "                zsum, zmix, zs, mix = accelerator.unwrap_model(aa_model).forward(stems,faders)\n",
    "                loss, log_dict = accelerator.unwrap_model(aa_model).loss(zsum, zmix)\n",
    "                accelerator.backward(loss)\n",
    "                opt.step()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if step % 25 == 0:\n",
    "                        tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')\n",
    "\n",
    "                    if use_wandb:\n",
    "                        log_dict = {\n",
    "                            **log_dict,\n",
    "                            'epoch': epoch,\n",
    "                            'loss': loss.item(),\n",
    "                            'lr': sched.get_last_lr()[0],\n",
    "                        }\n",
    "                        wandb.log(log_dict, step=step)\n",
    "\n",
    "                    if step % args.demo_every == 0:\n",
    "                        demo()\n",
    "\n",
    "                if step > 0 and step % args.checkpoint_every == 0:\n",
    "                    save()\n",
    "\n",
    "                step += 1\n",
    "            epoch += 1\n",
    "    except RuntimeError as err:  # ??\n",
    "            import requests\n",
    "            import datetime\n",
    "            ts = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            resp = requests.get('http://169.254.169.254/latest/meta-data/instance-id')\n",
    "            print(f'ERROR at {ts} on {resp.text} {device}: {type(err).__name__}: {err}', flush=True)\n",
    "            raise err\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234dda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed if listed in console_scripts in settings.ini\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
