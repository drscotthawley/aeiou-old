{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b736a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_audio_algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4bf95",
   "metadata": {},
   "source": [
    "# train_audio_algebra\n",
    "\n",
    "> Trying to map audio embeddings to vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f72a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from prefigure.prefigure import get_all_args, push_wandb_config\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import json\n",
    "\n",
    "import accelerate\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "#from torch.utils import data\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import wandb\n",
    "from shazbot.viz import embeddings_table, pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image\n",
    "import shazbot.blocks_utils as blocks_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df8512",
   "metadata": {},
   "source": [
    "## TagBox utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# TagBox Utils:\n",
    "#  Utilities from Ethan Manilows's TagBox: https://github.com/ethman/tagbox\n",
    "# slightly modified by Scott H. Hawley @drscotthawley\n",
    "\n",
    "#JUKEBOX_SAMPLE_RATE = 44100  # ethan's original\n",
    "JUKEBOX_SAMPLE_RATE = None\n",
    "\n",
    "def init_jukebox_sample_rate(sr=44100): # will probably use 48000 in practice\n",
    "    \"SHH added this util to preserve rest of code minimall-modified\"\n",
    "    global JUKEBOX_SAMPLE_RATE\n",
    "    JUKEBOX_SAMPLE_RATE = sr\n",
    "    return\n",
    "\n",
    "def audio_for_jbx(audio, trunc_sec=None, device=None):\n",
    "    \"\"\"Readies an audio TENSOR for Jukebox.\"\"\"\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio[None]\n",
    "        audio = audio.mean(axis=0)\n",
    "\n",
    "    # normalize audio\n",
    "    norm_factor = torch.abs(audio).max()\n",
    "    if norm_factor > 0:\n",
    "        audio /= norm_factor\n",
    "\n",
    "    if trunc_sec is not None:  # truncate sequence\n",
    "        audio = audio[: int(JUKEBOX_SAMPLE_RATE * trunc_sec)]\n",
    "\n",
    "    audio = audio[:, :, None]  # add one more dimension on the end?\n",
    "    return audio\n",
    "\n",
    "\n",
    "def load_audio_for_jbx(path, offset=0.0, dur=None, trunc_sec=None, device=None):\n",
    "    \"\"\"Loads a path for use with Jukebox.\"\"\"\n",
    "    audio, sr = librosa.load(path, sr=None, offset=offset, duration=dur)\n",
    "\n",
    "    if JUKEBOX_SAMPLE_RATE is None: init_jukebox_sample_rate()\n",
    "\n",
    "    if sr != JUKEBOX_SAMPLE_RATE:\n",
    "        audio = librosa.resample(audio, sr, JUKEBOX_SAMPLE_RATE)\n",
    "\n",
    "    return audio_for_jbx(audio, trunc_sec, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ada4ce",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# \"Icebox\": frozen Jukebox encoder for embeddings\n",
    "class IceBoxEncoder(nn.Module):\n",
    "    def __init__(self, global_args):\n",
    "        super().__init__()\n",
    "\n",
    "        n_io_channels = 2\n",
    "        n_feature_channels = 8\n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "        self.ema_decay = global_args.ema_decay\n",
    "\n",
    "        # for making Jukebox work with multi-GPU runs\n",
    "        rank, local_rank, device = int(os.getenv('RANK')), int(os.getenv('LOCAL_RANK')), self.device\n",
    "        dist_url = \"tcp://127.0.0.1:9500\"\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "\n",
    "        self.hps = Hyperparams()\n",
    "        assert global_args.sample_rate == 44100, \"Jukebox was pretrained at 44100 Hz.\"\n",
    "        self.hps.sr = global_args.sample_rate #44100\n",
    "        self.hps.levels = 3\n",
    "        self.hps.hop_fraction = [.5,.5,.125]\n",
    "\n",
    "        vqvae = \"vqvae\"\n",
    "        self.vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), self.device)\n",
    "        for param in self.vqvae.parameters():  # FREEZE IT.  \"IceBox\"\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.encoder = self.vqvae.encode\n",
    "        #self.encoder_ema = deepcopy(self.encoder)\n",
    "\n",
    "        latent_dim = 64 # global_args.latent_dim. Jukebox is 64\n",
    "        io_channels = 2#1 # 2.  Jukebox is mono but we decode in stereo\n",
    " \n",
    "    def encode(self, *args, **kwargs):\n",
    "        return self.encoder(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDK_DeleteMe(nn.Module):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        reals = batch[0]  # grab actual audio part of batch, not the filenames\n",
    "\n",
    "        #print(\"reals.size() = \",reals.size())\n",
    "        reals_mono = reals[:,0,:] # mono only, sorry\n",
    "        #print(\"reals_mono.size() = \",reals_mono.size())\n",
    "\n",
    "        encoder_input = audio_for_jbx(reals_mono, device=reals.device)\n",
    "\n",
    "        # Draw uniformly distributed continuous timesteps\n",
    "        t = self.rng.draw(reals.shape[0])[:, 0].to(self.device)\n",
    "\n",
    "        # Calculate the noise schedule parameters for those timesteps\n",
    "        alphas, sigmas = get_alphas_sigmas(get_crash_schedule(t))\n",
    "\n",
    "        # Combine the ground truth images and the noise\n",
    "        alphas = alphas[:, None, None]\n",
    "        sigmas = sigmas[:, None, None]\n",
    "        noise = torch.randn_like(reals)\n",
    "        noised_reals = reals * alphas + noise * sigmas\n",
    "        targets = noise * alphas - reals * sigmas\n",
    "\n",
    "        # Compute the model output and the loss.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            zs = self.encoder(encoder_input) # indices at 3 resolutions\n",
    "            xs = self.vqvae.bottleneck.decode(zs) # vectors vectors at 3 resolutions\n",
    "            tokens = self.package_3layer_tokens(xs).float() # combine resolutions\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = self.diffusion(noised_reals, t, tokens)\n",
    "            mse_loss = F.mse_loss(v, targets)\n",
    "            loss = mse_loss\n",
    "\n",
    "        log_dict = {\n",
    "            'train/loss': loss.detach(),\n",
    "            'train/mse_loss': mse_loss.detach(),\n",
    "        }\n",
    "\n",
    "        self.log_dict(log_dict, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def package_3layer_tokens(self, tokens_list):\n",
    "        \"jukebox vqvae returns a list of 3 1-dim tensor. Here we package them...somehow\"\n",
    "        ind = self.jukebox_layers[0]\n",
    "        return tokens_list[ind] # TODO: just grab one set of tokens for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376f45c",
   "metadata": {},
   "source": [
    "## The main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class AudioAlgebra(nn.Module):\n",
    "    def __init__(self, global_args, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = JukeBoxEncoder(global_args)\n",
    "\n",
    "\n",
    "    def loss(self, reals):\n",
    "\n",
    "        if self.pqmf_bands > 1:    \n",
    "            encoder_input = self.pqmf(reals)\n",
    "        else:\n",
    "            encoder_input = reals\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            latents = self.encoder(encoder_input).float()            \n",
    "\n",
    "        loss = 0# TODO: fill in mse_loss + mrstft_loss\n",
    "        \n",
    "        log_dict = {\n",
    "            'train/mse_loss': mse_loss.detach(),\n",
    "            'train/mb_distance': mb_distance.detach(),\n",
    "        }\n",
    "\n",
    "        return loss, log_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38edabc8",
   "metadata": {},
   "source": [
    "## Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0090bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def main():\n",
    "\n",
    "    args = get_all_args()\n",
    "    \n",
    "    args.random_crop = False\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method(args.start_method)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "     \n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    print('Using device:', device, flush=True)\n",
    "\n",
    "    aa_model = AudioAlgebra(args, device)\n",
    "\n",
    "    accelerator.print('Parameters:', blocks_utils.n_params(diffusion_model))\n",
    "\n",
    "    # If logging to wandb, initialize the run\n",
    "    use_wandb = accelerator.is_main_process and args.name\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        config = vars(args)\n",
    "        config['params'] = utils.n_params(diffusion_model)\n",
    "        wandb.init(project=args.name, config=config, save_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d74bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed if listed in console_scripts in settings.ini\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
