{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_audio_algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d264b",
   "metadata": {},
   "source": [
    "# train_audio_algebra\n",
    "\n",
    "> Trying to map audio embeddings to vector spaces\n",
    "\n",
    "Based on `accelerate`-powered code by Zach Evans & Katherine Crowson, cf. https://github.com/zqevans/audio-diffusion/blob/main/train_diffgan_accel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a29cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from prefigure.prefigure import get_all_args, push_wandb_config\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import json\n",
    "\n",
    "import accelerate\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "#from torch.utils import data\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import wandb\n",
    "from shazbot.viz import embeddings_table, pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image\n",
    "import shazbot.blocks_utils as blocks_utils\n",
    "from shazbot.icebox import load_audio_for_jbx, IceBoxEncoder\n",
    "from shazbot.data import MultiStemDataset\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0befb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# audio-diffusion imports\n",
    "from tqdm import trange\n",
    "import pytorch_lightning as pl\n",
    "from diffusion.pqmf import CachedPQMF as PQMF\n",
    "from diffusion.utils import PadCrop, Stereo, NormInputs\n",
    "from encoders.encoders import RAVEEncoder, ResConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6005f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#audio diffusion classes\n",
    "class DiffusionDVAE(pl.LightningModule):\n",
    "    def __init__(self, global_args, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "       \n",
    "        self.pqmf_bands = global_args.pqmf_bands\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            self.pqmf = PQMF(2, 70, global_args.pqmf_bands)\n",
    "\n",
    "        self.encoder = RAVEEncoder(2 * global_args.pqmf_bands, 64, global_args.latent_dim, ratios=[2, 2, 2, 2, 4, 4])\n",
    "        self.encoder_ema = deepcopy(self.encoder)\n",
    "        self.diffusion = DiffusionDecoder(global_args.latent_dim, 2)\n",
    "        self.diffusion_ema = deepcopy(self.diffusion)\n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "        #self.ema_decay = global_args.ema_decay\n",
    "        \n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "        if self.num_quantizers > 0:\n",
    "            quantizer_class = ResidualMemcodes if global_args.num_quantizers > 1 else Memcodes\n",
    "            \n",
    "            quantizer_kwargs = {}\n",
    "            if global_args.num_quantizers > 1:\n",
    "                quantizer_kwargs[\"num_quantizers\"] = global_args.num_quantizers\n",
    "\n",
    "            self.quantizer = quantizer_class(\n",
    "                dim=global_args.latent_dim,\n",
    "                heads=global_args.num_heads,\n",
    "                num_codes=global_args.codebook_size,\n",
    "                temperature=1.,\n",
    "                **quantizer_kwargs\n",
    "            )\n",
    "\n",
    "            self.quantizer_ema = deepcopy(self.quantizer)\n",
    "\n",
    "        \n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.encoder(*args, **kwargs)\n",
    "        return self.encoder_ema(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.diffusion(*args, **kwargs)\n",
    "        return self.diffusion_ema(*args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam([*self.encoder.parameters(), *self.diffusion.parameters()], lr=2e-5)\n",
    "\n",
    "  \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        reals = batch[0]\n",
    "\n",
    "        encoder_input = reals\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            encoder_input = self.pqmf(reals)\n",
    "        \n",
    "        # Draw uniformly distributed continuous timesteps\n",
    "        t = self.rng.draw(reals.shape[0])[:, 0].to(self.device)\n",
    "\n",
    "        # Calculate the noise schedule parameters for those timesteps\n",
    "        alphas, sigmas = get_alphas_sigmas(get_crash_schedule(t))\n",
    "\n",
    "        # Combine the ground truth images and the noise\n",
    "        alphas = alphas[:, None, None]\n",
    "        sigmas = sigmas[:, None, None]\n",
    "        noise = torch.randn_like(reals)\n",
    "        noised_reals = reals * alphas + noise * sigmas\n",
    "        targets = noise * alphas - reals * sigmas\n",
    "\n",
    "        # Compute the model output and the loss.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            tokens = self.encoder(encoder_input).float()\n",
    "\n",
    "        if self.num_quantizers > 0:\n",
    "            #Rearrange for Memcodes\n",
    "            tokens = rearrange(tokens, 'b d n -> b n d')\n",
    "\n",
    "            #Quantize into memcodes\n",
    "            tokens, _ = self.quantizer(tokens)\n",
    "\n",
    "            tokens = rearrange(tokens, 'b n d -> b d n')\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = self.diffusion(noised_reals, t, tokens)\n",
    "            mse_loss = F.mse_loss(v, targets)\n",
    "            loss = mse_loss\n",
    "\n",
    "        log_dict = {\n",
    "            'train/loss': loss.detach(),\n",
    "            'train/mse_loss': mse_loss.detach(),\n",
    "        }\n",
    "\n",
    "        self.log_dict(log_dict, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "        '''def on_before_zero_grad(self, *args, **kwargs):\n",
    "        decay = 0.95 if self.current_epoch < 25 else self.ema_decay\n",
    "        ema_update(self.diffusion, self.diffusion_ema, decay)\n",
    "        ema_update(self.encoder, self.encoder_ema, decay)\n",
    "\n",
    "        if self.num_quantizers > 0:\n",
    "            ema_update(self.quantizer, self.quantizer_ema, decay)'''\n",
    "    \n",
    "    def setup_weights(self):\n",
    "        pthfile = 'audio-diffusion.pth'\n",
    "        cmd = 'curl -C - -LO https://www.dropbox.com/s/8tcirpokhoxfo82/dvae-checkpoint-june9.pth; [ ! -f \"{pthfile}\" ] && cp dvae-checkpoint-june9.pth {pthfile}'\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "        self.load_state_dict(torch.load(pthfile))\n",
    "        self = self.to(self.device)\n",
    "\n",
    "def ad_encode_it(reals, device, encoder_ema, quantizer_ema, sample_size=32768, num_quantizers=8):\n",
    "    encoder_input = reals.to(device)\n",
    "    noise = torch.randn([reals.shape[0], 2, sample_size]).to(device)\n",
    "\n",
    "    tokens = encoder_ema(encoder_input)\n",
    "    if num_quantizers > 0:\n",
    "        #Rearrange for Memcodes\n",
    "        tokens = rearrange(tokens, 'b d n -> b n d')\n",
    "        tokens, _= quantizer_ema(tokens)\n",
    "        tokens = rearrange(tokens, 'b n d -> b d n')\n",
    "\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d7d92",
   "metadata": {},
   "source": [
    "## The main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, dims:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dims, dims, **kwargs)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.bn = nn.BatchNorm1d(dims)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.lin(x)\n",
    "        x = self.bn(x)\n",
    "        return F.leaky_relu(x, inplace=True)\n",
    "    \n",
    "\n",
    "class AudioAlgebra(nn.Module):\n",
    "    def __init__(self, global_args, device, encoder):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = encoder\n",
    "        self.dims = global_args.latent_dim\n",
    "        \n",
    "        self.reembedding = nn.Sequential(  # something simple at first\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            nn.Linear(self.dims,self.dims)\n",
    "            )\n",
    "        \n",
    "    def forward(self, \n",
    "        stems:list,   # list of torch tensors denoting (chunked) solo audio parts to be mixed together\n",
    "        faders:list   # list of gain values to be applied to each stem\n",
    "        ):\n",
    "        \"\"\"We're going to 'on the fly' mix the stems according to the fader settings and generate \n",
    "        frozen-encoder embeddings for each (fader-adjusted) stem and for the total mix.\n",
    "        \"z0\" denotes an embedding from the frozen encoder, \"z\" denotes re-mapped embeddings \n",
    "        in (hopefully) the learned vector space\"\"\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            zs, zsum = [], torch.zeros((self.dims)).float()\n",
    "            mix = torch.zeros_like(stems[0]).float()\n",
    "            for s, f in zip(stems, faders):\n",
    "                mix_s = s * f             # audio stem adjusted by gain fader f\n",
    "                with torch.no_grad():\n",
    "                    z0 = self.encoder.encode(mix_s).float()  # initial/frozen embedding/latent for that input\n",
    "                z = self.reembedding(z0).float()   # <-- this is the main work of the model \n",
    "                zsum += z                 # compute the sum of all the z's. we'll end up using this in our (metric) loss as \"pred\"\n",
    "                mix += mix_s              # save a record of full audio mix\n",
    "                zs.append(z)              # save a list of individual z's\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                zmix0 = self.encoder.encode(mix).float()  # compute frozen embedding / latent for the full mix\n",
    "            zmix = self.reembedding(zmix0).float()        # map that according to our learned re-embedding. this will be the \"target\" in the metric loss\n",
    "            \n",
    "        return zsum, zmix, zs, mix    # zsum = pred, zmix = target, and zs & zmix are just for extra info\n",
    "        \n",
    "        \n",
    "    def distance(self, pred, targ):\n",
    "            return torch.norm( pred - targ ) # L2 / Frobenius / Euclidean\n",
    "        \n",
    "    def loss(self, zsum, zmix):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = distance(zsum, zmix)\n",
    "        log_dict = {'loss': loss.detach()}\n",
    "        return loss, log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c62057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def demo():\n",
    "    print(\"In demo placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c706226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def get_stems_faders(batch, dl):\n",
    "    \"grab some more audio stems and set faders\" \n",
    "    nstems = 1 + int(torch.randint(5,(1,1))[0][0].numpy())\n",
    "    faders = 2*torch.rand(nstems)-1  # fader gains can be from -1 to 1\n",
    "    stems = [batch]\n",
    "    dl_iter = iter(dl)\n",
    "    for i in range(nstems-1):\n",
    "        stems.append(next(dl_iter))\n",
    "    return stems, faders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def save(args, model, opt, epoch, step):\n",
    "    \"checkpointing\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    filename = f'{args.name}_{step:08}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        tqdm.write(f'Saving to {filename}...')\n",
    "    obj = {\n",
    "        'model': accelerator.unwrap_model(model).state_dict(),\n",
    "        'opt': opt.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'step': step\n",
    "    }\n",
    "    accelerator.save(obj, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62783775",
   "metadata": {},
   "source": [
    "## Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40429a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def main():\n",
    "\n",
    "    args = get_all_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method(args.start_method)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "     \n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    print('Using device:', device, flush=True)\n",
    "\n",
    "    encoder_choices = ['ad','icebox']\n",
    "    encoder_choice = encoder_choice[0]\n",
    "    print(f\"Using {encoder_choice} as encoder\")\n",
    "    if 'icebox' == encoder_choice:\n",
    "        encoder = IceBoxEncoder(args, device)\n",
    "    elif 'ad' == encoder_choice:\n",
    "        dvae = DiffusionDVAE(args, device)\n",
    "        dvae.setup_weights()\n",
    "        encoder = ADEncoder(args,device,dl_weights=True)\n",
    "    \n",
    "    print(\"Setting up AA model\")\n",
    "    aa_model = AudioAlgebra(args, device, encoder)\n",
    "\n",
    "    accelerator.print('AA Model Parameters:', blocks_utils.n_params(aa_model))\n",
    "\n",
    "    # If logging to wandb, initialize the run\n",
    "    use_wandb = accelerator.is_main_process and args.name\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        config = vars(args)\n",
    "        config['params'] = utils.n_params(aa_model)\n",
    "        wandb.init(project=args.name, config=config, save_code=True)\n",
    "\n",
    "    opt = optim.Adam([*aa_model.reembedding.parameters()], lr=4e-5)\n",
    "\n",
    "    train_set = MultiStemDataSet([args.training_dir], args)\n",
    "    train_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=True,\n",
    "                               num_workers=args.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "    aa_model, opt, train_dl = accelerator.prepare(aa_model, opt, train_dl)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.watch(aa_model)\n",
    "        \n",
    "    if args.ckpt_path:\n",
    "        ckpt = torch.load(args.ckpt_path, map_location='cpu')\n",
    "        accelerator.unwrap_model(aa_model).load_state_dict(ckpt['model'])\n",
    "        opt.load_state_dict(ckpt['opt'])\n",
    "        epoch = ckpt['epoch'] + 1\n",
    "        step = ckpt['step'] + 1\n",
    "        del ckpt\n",
    "    else:\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "\n",
    "    # all set up, let's go\n",
    "    try:\n",
    "        while True:  # training loop   \n",
    "            for batch in tqdm(train_dl, disable=not accelerator.is_main_process):\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                # \"batch\" is actually not going to have all the data we want. We could rewrite the dataloader to fix this,\n",
    "                # but instead I just added get_stems_faders() which grabs \"even more\" audio to go with \"batch\"\n",
    "                stems, faders = get_stems_faders(batch, train_dl)\n",
    "                \n",
    "                zsum, zmix, zs, mix = accelerator.unwrap_model(aa_model).forward(stems,faders)\n",
    "                loss, log_dict = accelerator.unwrap_model(aa_model).loss(zsum, zmix)\n",
    "                accelerator.backward(loss)\n",
    "                opt.step()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if step % 25 == 0:\n",
    "                        tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')\n",
    "\n",
    "                    if use_wandb:\n",
    "                        log_dict = {\n",
    "                            **log_dict,\n",
    "                            'epoch': epoch,\n",
    "                            'loss': loss.item(),\n",
    "                            'lr': sched.get_last_lr()[0],\n",
    "                        }\n",
    "                        wandb.log(log_dict, step=step)\n",
    "\n",
    "                    if step % args.demo_every == 0:\n",
    "                        demo()\n",
    "\n",
    "                if step > 0 and step % args.checkpoint_every == 0:\n",
    "                    save(args, aa_model, opt, epoch, step)\n",
    "\n",
    "                step += 1\n",
    "            epoch += 1\n",
    "    except RuntimeError as err:  # ??\n",
    "        import requests\n",
    "        import datetime\n",
    "        ts = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        resp = requests.get('http://169.254.169.254/latest/meta-data/instance-id')\n",
    "        print(f'ERROR at {ts} on {resp.text} {device}: {type(err).__name__}: {err}', flush=True)\n",
    "        raise err\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ff373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Not needed if listed in console_scripts in settings.ini\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():  # don't execute in notebook\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a2e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
